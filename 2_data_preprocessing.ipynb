{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before feeding textual data to a ML model, it needs to be carefully preprocessed, otherwise the results will be poor. This is especially true for short, noisy data, such as user reviews.\n",
    "\n",
    "My data preprocessing consisted of the following steps:\n",
    "\n",
    "- **Filtering out uninformative reviews**\n",
    "- **Removing English and corpus-specific stopwords**\n",
    "- **Basic spellcheck**\n",
    "- **Lemmatization**\n",
    "\n",
    "I am going to illustrate this technique using the investing app dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filtering out Uninformative Reviews\n",
    "\n",
    "Reviews vary in quality. Some are extremely short and contain uninformative feedback for app developers (e.g., *This app sucks!*), while others are long and contain valuable information in the form of bug reports and feature requests. \n",
    "\n",
    "There are plethora of methods in the literature to filter out uninformative feedback. However, most of them rely on supervised techniques, which is not feasible for the current task. To keep things simple, we use the approach from [this paper](https://ieeexplore.ieee.org/abstract/document/9283933?casa_token=fG8wgt1iNNoAAAAA:YZbVCUM0kDatSSphbQdbs8Su4KeoqRe9kJF_oLSxF-q_Mfbzln9WBgJ7ZQG_sauC9OrpevhBCQ). The main idea behind this approach is to look for indicative keywords. Reviews that contain such keywords are more likely to be informative. Such a method allows us to quickly filter out uninformative feedback with minimum effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    688290\n",
      "True      20760\n",
      "Name: for_analysis, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv('investing.csv')\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# indicative keywords\n",
    "markers = ['after', 'as soon as', 'before', 'every time', 'then', 'until', 'when', 'whenever', 'while', 'during'] \n",
    "\n",
    "markers_re = r\"\\b(\" + '|'.join(markers) + r\")\\b\"\n",
    "\n",
    "# this column marks informative reviews\n",
    "data['for_analysis'] = False\n",
    "data['for_analysis'] = (data['content'].str.contains(markers_re, regex=True) & (data['score'] <= 2) & (data['date'] <= '2021-01-01') & (data.app.isin(data.app.value_counts()[data.app.value_counts() > 10000].index)))\n",
    "\n",
    "print(data['for_analysis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**~20k** reviews out of **688k** contained the indicative keywords. This number can be increased by identifying a larger set of keywords (for the future work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "app            \n",
       "robinhood          7879\n",
       "acorn              4343\n",
       "stash              2445\n",
       "etrade             1605\n",
       "fidelity           1497\n",
       "tdameritrade       1403\n",
       "schwab             1079\n",
       "personalcapital     509\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['for_analysis'] == True,['app']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Removing English and Corpus-Specific Stopwords\n",
    "\n",
    "To improve the performance of the NLP algorithm, I remove the common English stop-words from the reviews using **NLTK**. In addition, I remove corpus-specific stop words. Corpus-specific stop-words can be identified at a later stage after performing the modeling and examining the topics. Such stop-words do not add any useful information to the topic interpretation. Examples include *app*, *make*, and *get*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Spellcheck\n",
    "\n",
    "User reviews are very informal and contain colloquial langauge. Therefore, I added an additional step to my NLP pipeline in the form of a basic spelling correction. Examples of it include *ppl*->*people* and *hrs*->*hours*. I found that such corrections increased the topic quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization\n",
    "\n",
    "Different word forms can convey essentially the same meaning (e.g., *work* and *working*). However, the topic model might not necessarily be able to infer that information, especially when the frequency of one form or the other is too low. Therefore, as a final step, the different word forms need to be collapsed into single, most general ones. \n",
    "\n",
    "There are two main wasy to do that: stemming and lemmatization. Stemming works by extracting the root form of the word and is commonly performed using PorterStemmer. The main disadvantage of stemming is lost information: for example, the word *argue* would become *argu*, which is not very interpretable. Lemmatization, on the other hand, converts the words into their dictionary form: *arguing* would become *argue*. The main disadvantage of lemmatiation over stemming is that lemmatization produces more words with similar meanings in the context of topic modeling. However, the main advantage is that topics become more interpretable.\n",
    "\n",
    "After experimenting with both stemming and lemmatization I found that lemmatization produces dignificantly better topics. In the next cell I combine steps 2-4 into a single pipeline to produce the final review cohort for the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "wordnet_map = {\n",
    "    \"N\": wordnet.NOUN,\n",
    "    \"V\": wordnet.VERB,\n",
    "    \"J\": wordnet.ADJ,\n",
    "    \"R\": wordnet.ADV,\n",
    "    \"S\": wordnet.ADJ_SAT\n",
    "}\n",
    "\n",
    "# app names were added to stop word list to enhance generalizability\n",
    "cohort_stopwords = data['app'].value_counts().index.tolist() + ['fargo'] + ['capitalone']\n",
    "grammar_fix = {}\n",
    "\n",
    "# adding the rest of the stop words\n",
    "with open('stop_words.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        s = line.split()\n",
    "        if len(s) <= 1 and s[0] != '\\n':\n",
    "            cohort_stopwords.append(s[0])\n",
    "        elif len(s) > 1 and s[0] != '\\n':\n",
    "            grammar_fix[s[0]] = \" \".join([w for w in s[1:]])\n",
    "\n",
    "lem = WordNetLemmatizer()            \n",
    "\n",
    "def preprocess(text):\n",
    "    try:\n",
    "        text = re.sub(\"\\$\", \" money \", text)\n",
    "        text = re.sub(r'(\\d+)(\\w+)', r'\\1 \\2', text) #2hrs = 2 hrs\n",
    "        words = [word.lower() for word in word_tokenize(text) if word.isalpha()]\n",
    "        words = [grammar_fix[word] if word in grammar_fix else word for word in words]\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        words = [lem.lemmatize(word, wordnet_map.get(nltk.pos_tag([word])[0][1][0], wordnet.NOUN)) for word in words]\n",
    "        words = [word for word in words if word not in cohort_stopwords]\n",
    "        return words\n",
    "    except:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['take', 'hour', 'food', 'say', 'minute']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"it takes 2hrs just to get my food. when it says 30mins.. don't get this app.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing all of the texts\n",
    "texts = data.loc[data['for_analysis'], 'content'].values.tolist()\n",
    "pr_texts = [preprocess(text) for text in texts]\n",
    "data.loc[data['for_analysis'], 'content_processed'] = [\" \".join(txt) for txt in pr_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687369                                                                                                                                                                                                                                                                 download curiosity decides month later interact take fee bank account never investment plan question wait month inactive start charge fee email\n",
       "309480    since around good start invest learn monthly fee ridiculous broker free set forget mentality isnt anymore either master option trading point normal broker sell equity right away price sell stuff random price day scam trading window stock etf absolutely reason trading like mutual fund yearly cost worth price give ebb flow market honestly waste time money slowly grown think swim want pay free do\n",
       "281583                                                                                                                                                                                    champion force upgrade gold want buy sell increase buying power etc seem like bug reach tothem sent support email tweet support twitter acct help clear able take action unless upgrade paid option still free stock trading\n",
       "668512                                                                                                                                                                                                                                                                                               picture extremely blurry ca clear shot check actually take picture clear everything else irrelevant issue correct\n",
       "629194                                                                                                                                                                                                                                                                                                                                              stole share miss buck bought stock cent like left like month share\n",
       "Name: content_processed, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['for_analysis'], 'content_processed'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the resulted texts into csv\n",
    "data.content_processed = data.content_processed.fillna(\"\")\n",
    "data.loc[(data.content_processed.apply(len) == 0), 'for_analysis'].for_analysis = False\n",
    "data.to_csv('dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
